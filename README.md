
# Sample - Large Language Model (From Scratch)

This repository demonstrates how a basic character-level language model pipeline works using a very small dataset.

We manually trace how the text:

"hello world"

is processed step-by-step through the pipeline.

---

##  Pipeline Overview

Text
â†“
Tokenization
â†“
Integer Mapping
â†“
Embedding Vectors
â†“
Dense Output (Logits)
â†“
Probability Distribution (Softmax)

---

##  Dataset and Input

Dataset:
"hello world"

Model Input:
"hello"

Goal:
Given "hello" â†’ Predict the next character.

---

## ðŸ”¹ Step 1 â€” Vocabulary Creation

Unique characters in "hello world":

{'h', 'e', 'l', 'o', ' ', 'w', 'r', 'd'}

After sorting:

[' ', 'd', 'e', 'h', 'l', 'o', 'r', 'w']

Vocabulary size:
8

Character â†’ Index Mapping:

' ' â†’ 0  
'd' â†’ 1  
'e' â†’ 2  
'h' â†’ 3  
'l' â†’ 4  
'o' â†’ 5  
'r' â†’ 6  
'w' â†’ 7  

---

## ðŸ”¹ Step 2 â€” Convert Input to Integers

Input:
"hello"

Conversion:

h â†’ 3  
e â†’ 2  
l â†’ 4  
l â†’ 4  
o â†’ 5  

Integer sequence:
[3, 2, 4, 4, 5]

Tensor shape:
(1, 5)

Explanation:
1 â†’ Batch size  
5 â†’ Sequence length  

Tensor representation:
[[3, 2, 4, 4, 5]]

---

## ðŸ”¹ Step 3 â€” Embedding Layer

Embedding dimension:
10

Embedding Matrix Shape:
(8, 10)

Meaning:
8 rows â†’ one per character  
10 columns â†’ embedding size  

Each integer is replaced with its corresponding embedding vector.

Input:
[3, 2, 4, 4, 5]

Becomes:

[
 embedding(h),
 embedding(e),
 embedding(l),
 embedding(l),
 embedding(o)
]

Output shape:
(1, 5, 10)

---

## ðŸ”¹ Step 4 â€” Dense Layer (Prediction Layer)

Each 10-dimensional embedding is transformed into an 8-dimensional output vector (vocab size = 8).

Output shape:
(1, 5, 8)

For each input token, the model predicts scores for all 8 possible next characters.

---

## ðŸ”¹ Step 5 â€” Last Token Selection

We select the last token ("o") output.

Shape:
(8,)

This represents:

"Given 'hello', what is the next character?"

These values are called logits (raw scores).

---

## ðŸ”¹ Step 6 â€” Softmax

Softmax converts logits into probabilities:

P(i) = e^(logit_i) / Î£ e^(logit_j)

Properties:
â€¢ Values between 0 and 1  
â€¢ Sum of probabilities = 1  

Example (random since untrained):

' ' â†’ 0.18  
'd' â†’ 0.03  
'e' â†’ 0.05  
'h' â†’ 0.07  
'l' â†’ 0.12  
'o' â†’ 0.09  
'r' â†’ 0.22  
'w' â†’ 0.24  

---

## ðŸ”¹ Step 7 â€” Argmax

Highest probability index â†’ 7  

Index 7 â†’ 'w'

Final prediction:

"hello" â†’ next character = "w"

---

##  Main Aim:

â€¢ How text becomes numbers  
â€¢ How embeddings convert tokens into vector space  
â€¢ How neural networks produce probability distributions  
â€¢ Core mechanics behind language models  

Even large language models follow the same fundamental principle â€” just at massive scale.
